# ğŸ“ˆ Real-time Bitcoin Data Processing with PySpark

> **Course**: DATA605 â€“ Spring 2025  
> **Project Title**: Real-time Bitcoin Data Processing with PySpark  
> **Student**: Venkata Siva Rajesh Vithanala  
> **Difficulty**: 3

---

## ğŸš€ Objective

This project demonstrates how to use **Apache Spark (PySpark)** to ingest, process, analyze, and visualize **real-time Bitcoin price data** from the CoinGecko API using a complete **Dockerized pipeline**.

Students and beginners can use this tutorial to learn how to:
- Ingest streaming data
- Apply transformations and aggregations
- Run time-series machine learning (GBT Regression)
- Save results to S3
- Visualize predictions vs actuals

---




## ğŸš€ What is PySpark?

**PySpark** is the Python API for Apache Spark, a fast and general-purpose engine for large-scale data processing. It enables parallel computation on large datasets across distributed computing environments.

### âœ… Key Features of PySpark:
- Distributed data processing with **RDDs** and **DataFrames**
- Real-time stream processing with **Spark Streaming**
- Built-in **MLlib** for scalable machine learning
- SQL-like querying for structured data
- Easily integrates with cloud storage (e.g., **AWS S3**, **HDFS**)

---

## ğŸ§± Use Cases of PySpark
- Real-time analytics (e.g., IoT, finance)
- Machine learning on big datasets
- ETL pipelines for massive structured/unstructured data
- Batch jobs over petabytes of data
- Log analysis, clickstream processing, and more

---


## ğŸ“Š Architecture Overview

![Architecture Diagram](work/assets/architecture_diagram.png)



## ğŸ”§ Tech Stack

| Component | Description |
|----------|-------------|
| **PySpark** | Distributed data processing and MLlib for time-series regression |
| **Spark Streaming** | Real-time ingestion of Bitcoin data |
| **CoinGecko API** | Source for Bitcoin OHLC data |
| **Boto3** | Upload results to AWS S3 |
| **Matplotlib** | Plot predicted vs actual prices |
| **Docker + Jupyter** | Portable, reproducible environment |

---

## ğŸ§± Project Structure

```bash
.
â”œâ”€â”€ work/
â”‚   â”œâ”€â”€ bitcoin_utils.py         # All reusable logic and API calls
â”‚   â”œâ”€â”€ run_pipeline.py          # End-to-end pipeline that runs automatically
â”‚   â”œâ”€â”€ Spring2025.API.ipynb     # API usage walkthrough
â”‚   â”œâ”€â”€ Spring2025.example.ipynb # End-to-end example run
â”‚   â”œâ”€â”€ Spring2025.API.md        # Markdown doc for API
â”‚   â”œâ”€â”€ Spring2025.example.md    # Markdown doc for example
â”œâ”€â”€ Dockerfile                   # Spark + Jupyter container
â”œâ”€â”€ docker-compose.yml           # Mounts, ports, and service config
â”œâ”€â”€ .env.example                 # Sample env (exclude real credentials)
â”œâ”€â”€ requirements.txt             # Python packages
```

---

## ğŸ” Workflow Overview

```mermaid
graph LR
    A[fetch_price_as_ohlc()] --> B[start_file_producer()]
    B --> C[Stream files to /Data/stream/]
    C --> D[PySpark Streaming reads JSON]
    D --> E[Aggregate + Filter data]
    E --> F[GBTRegressor model]
    F --> G[Evaluate RÂ² and RMSE]
    G --> H[Save as Parquet and ZIP]
    H --> I[Upload to AWS S3 (if env vars set)]
    I --> J[Plot actual vs predicted ğŸ“‰]
```

---

## â–¶ï¸ How to Run

### 1. Clone the Repo

```bash
git clone https://github.com/SivaRajes/RealTimeBitcoinProcessingUsingPyspark.git



```

### 2. Prepare `.env` File

```bash
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=us-east-2
S3_BUCKET_NAME=your_bucket_name

# Then edit it to include your own AWS credentials and bucket name.
```

### 3. Build and Run Dockerized Spark + Jupyter

```bash
docker-compose up --build
```

This will:
- Start pipeline.py
- Ingest Bitcoin prices (90 seconds)
- Run hourly/daily/moving aggregations
- Train GBTRegressor
- Upload results to S3
- Plot the actual vs predicted chart



## âœ… Outputs
- writes stream of json files to the already existed history data
- Spark stream and parses json to Spark 
- prints Aggregation tables (hourly, daily, moving averages)
- Machine Learning Forecasts (GBTRegressor)
- RMSE and RÂ² metrics
- Parquet file and zipped archive
- Upload to AWS S3
- Visualization plot (`output_plot.png`)

---

## ğŸ“š Documentation

| File | Purpose |
|------|---------|
| `Spring2025.API.md` | Details the helper functions and Spark logic |
| `Spring2025.example.md` | Shows full usage with output |
| `bitcoin_utils.py` | All functions: fetch, stream, ML, upload |
| `run_pipeline.py` | Runs everything automatically |

---

## ğŸ“ Notes from Instructor

âœ” No need to share credentials â€” make sure `.env` can be easily edited    

---

## ğŸ“ Useful Commands

```bash
# Check container logs
docker-compose logs -f

# Rebuild container after changes
docker-compose up --build

# Clean up orphan containers
docker-compose down --remove-orphans
```

---



## ğŸ§  Learn More

- [Apache Spark Docs](https://spark.apache.org/docs/latest/)
- [PySpark MLlib Guide](https://spark.apache.org/docs/latest/ml-guide.html)
- [CoinGecko API](https://www.coingecko.com/en/api)
